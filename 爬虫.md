## 入门

### Robots协议

Robots协议（爬虫协议）的全称是“网络爬虫排除标准”（Robots Exclusion Protocol），网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。该协议是国际互联网界通行的道德规范，虽然没有写入法律，但是每一个爬虫都应该遵守这项协议。

### 爬虫与反爬虫

过于快速或者频密的网络爬虫都会对服务器产生巨大的压力，网站可能封锁你的IP，甚至采取进一步的法律行动。因此，你需要约束自己的网络爬虫行为，将请求的速度限定在一个合理的范围之内。

### 流程

网络爬虫的流程其实非常简单，主要可以分为三部分：

1.  获取网页
   1. 基础：urllib、requests、selenium
   2. 进阶：多进程多线程抓取、登录抓取、突破IP封禁、使用服务器抓取
2. 解析网页（提取数据）
   1. 基础：re正则表达式、BeautifulSoup、lxml、xpath
   2. 进阶：解决中文乱码
3. 存储数据
   1. 基础：存入文件
   2. 进阶：存入MySQL

## 静态网页抓取

### Requests库

Requests库能够让你轻易地发送HTTP请求，比自带的urllib库方便多了。

```python
import requests

# 发送get请求，参数是params
param_dict = {'wd': '美女'}
headers = {"User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                          "(KHTML, like Gecko) Chrome/94.0.4606.61 Safari/537.36"),
           "Accept": ("text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,"
                      "image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9")}
r = requests.get('http://www.baidu.com/s', params=param_dict, headers=headers)

print("状态码：", r.status_code)
print('编码：', r.encoding)
print("响应体：", r.text)

# 发送post请求，参数使用data关键字
r2 = requests.post('http://www.baidu.com/s', data=param_dict)
print(r2.text)

```

（1）r.text是服务器响应的内容，会自动根据响应头部的字符编码进行解码。

（2）r.encoding是服务器内容使用的文本编码。

（3）r.status_code用于检测响应的状态码

（4）r.content是字节方式的响应体，会自动解码gzip和deflate编码的响应数据。

（5）r.json()是Requests中内置的JSON解码器。

## 动态网页抓取

由于主流网站使用JavaScript展现网页内容，和静态网页不同的是，使用JavaScript时，很多内容并不会出现在HTML源代码中，所以爬取静态网页的技术可能无法正常使用。因此，我们需要用到动态网页抓取的两种技术：通过浏览器审查元素解析真实网页地址和使用Selenium模拟浏览器的方法。

动态网页大量使用Ajax技术，怎么爬取里面动态加载的内容呢？有两种方法：（1）通过浏览器审查元素解析地址。（2）通过Selenium模拟浏览器抓取。

F12的network界面可以选择xhr（XMLHttpRequest），异步的请求一般在xhr中。还有一些放在js中，是历史遗留的问题，因为有些浏览器不支持异步请求。

```python
import requests

# 爬取豆瓣喜剧电影排行榜
url = "https://movie.douban.com/j/chart/top_list"
# get请求的url太长了，就分开，下面是query string
data = {
    "type": '24',
    'interval_id': '100:90',
    'action': '',
    'start': 0,
    'limit': 20
}
# 简单的反爬
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.61 Safari/537.36"
}
resp = requests.get(url=url, params=data, headers=headers)
print(resp.json())
# 关闭和浏览器的连接
resp.close()
```

## 数据解析

### re

基本语法：

```python
import re

# findall返回列表
list1 = re.findall(r'\d+', '电信客户请拨打10086或10010')
print(list1)

# finditer返回迭代器，迭代器的数据用group()获取
iter = re.finditer(r'\d+', '电信客户请拨打10086或10010')
for i in iter:
    print(i.group())

# search返回match对象，匹配一个就结束。通过group()拿数据。
search1 = re.search(r'\d+', '电信客户请拨打10086或10010')
print(search1.group())

# match和search类似，但是从头匹配
match1 = re.match(r'\d+', '电信客户请拨打10086或10010')  # NoneType，什么都没匹配上

# 预加载正则表达式，速度快，可以重复用
expr = re.compile(r'\d+')
expr.findall('电信客户请拨打10086或10010')

s = """
<div class='jay'><span id='1'>周杰伦</span></div>
<div class='jj'><span id='2'>林俊杰</span></div>
<div class='java'><span id='3'>甲骨文</span></div>
"""
# (?P<组名> re表达式) 分组匹配
expr2 = re.compile(r"<div class='.*?'><span id='(?P<id>\d+)'>(?P<name>.*?)</span></div>", re.S)  # re.S让.能匹配换行符
result = expr2.finditer(s)
for i in result:
    print("id: ", i.group("id"))
    print("name: ", i.group("name"))

```

实例：爬取豆瓣排行榜

```python
import requests
import re
import csv

# 获取数据
url = 'https://movie.douban.com/top250'
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.61 Safari/537.36"
}
resp = requests.get(url, headers=headers)
page_content = resp.text
# 解析数据
expr = re.compile(r'<li>.*?<span class="title">(?P<title>.*?)'
                  r'</span>.*?<p class="">.*?<br>(?P<year>.*?)&nbsp'
                  r'.*?<span class="rating_num" property=".*?">'
                  r'(?P<score>.*?)</span>', re.S)
result = expr.finditer(page_content)

# 保存数据
f = open("top250.csv", mode="w", encoding='utf-8')
csvwriter = csv.writer(f)
for item in result:
    dic = item.groupdict()
    dic['year'] = dic['year'].strip()
    csvwriter.writerow(dic.values())

f.close()
print("over")
```

### bs4

```python
import requests
from bs4 import BeautifulSoup
import csv

# 获取响应数据
url = "http://top.100ppi.com/"
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.61 Safari/537.36"
}
resp = requests.get(url, headers=headers)
source = resp.text

# 解析请求数据
# 1.源码交给BeautifulSoup处理，生成bs对象
page = BeautifulSoup(source, "html.parser")  # 指定html解析器
# 2.从bs对象中查找数据
# find(标签，属性=值) 查找第一个
# find_all 查找所有

div = page.find_all("div", class_='dzb')[1]
table = div.find_all("ul", class_='dzb-txt')

# 保存数据
f = open("大宗商品涨跌榜.csv", 'w', encoding="utf-8", newline='')
csvwriter = csv.writer(f)
csvwriter.writerow(["大宗榜名","涨占比","跌占比","均涨幅","指数涨幅"])
for it in table:
    datas = it.find_all("li")
    name = datas[0].text
    up_percent = datas[1].text
    down_percent = datas[2].text
    avg_up = datas[3].text.strip()
    index_up = datas[4].text.strip()
    csvwriter.writerow([name, up_percent, down_percent, avg_up, index_up])

f.close()
print("over")
```

